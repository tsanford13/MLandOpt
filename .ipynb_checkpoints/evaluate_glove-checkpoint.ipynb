{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A BATS_Analogy is an analogy from the BATS test set. It may allow\n",
    "multiple correct answers! While most analogies are of the form\n",
    "a:b::c:d (read as \"a is to be as c is to d\") a BATS Analogy is of the form\n",
    "a:b1,b2,...,bn::c:d,d1,d2,...,dn (read as \"a is to b1 or b2 or ... or bn as \n",
    "c is to d1 or d2 or ... or dn\") an example would be:\n",
    "\"snake is to nest/pit/acquarium as tiger is to den/cage.\"'''\n",
    "class BATS_Analogy:\n",
    "    def __init__(self, a, b_set, c, d_set):\n",
    "        self.a = a\n",
    "        self.b_set = b_set\n",
    "        self.c = c\n",
    "        self.solution_set = d_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''takes two matrices A and B and returns \n",
    "A@B. If A and B are normalized then this is equivalent\n",
    "to calculating the cosine similarity of each row of A with\n",
    "each column of B. i.e. the cosine similarity of the ith row of A\n",
    "with the jth column of B is given by (A@B)[i,j]'''\n",
    "def cosine_similarity_normalized(A, B):\n",
    "    return A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_accuracy(analogies, embeddings, words_to_indices, indices_to_words, sim):\n",
    "    num_correct = 0\n",
    "    analogy_predictions_indices = dict()\n",
    "    start = 0\n",
    "    all_prediction_matrices = []\n",
    "    num_predictions = 0\n",
    "    for analogy in analogies:\n",
    "        num_predictions += len(analogy.b_set)\n",
    "        end = start + len(analogy.b_set)\n",
    "        analogy_predictions_indices[analogy] = (start,end)\n",
    "        start = end\n",
    "       # temp = np.empty((len(analogy.b_set), embeddings.shape[1]), dtype=np.float32)\n",
    "        \n",
    "    all_predictions = np.empty((num_predictions, embeddings.shape[1]), dtype=np.float32)\n",
    "    for analogy in analogies:\n",
    "        offset = analogy_predictions_indices.get(analogy)[0]\n",
    "        for i in range(analogy_predictions_indices.get(analogy)[1] - offset):\n",
    "            all_predictions[i + offset] = embeddings[words_to_indices.get(analogy.b_set[i])]\n",
    "        start = analogy_predictions_indices.get(analogy)[0]\n",
    "        end = analogy_predictions_indices.get(analogy)[1]\n",
    "        all_predictions[range(start, end)] += embeddings[words_to_indices.get(analogy.c)]\n",
    "        all_predictions[range(start, end)] -= embeddings[words_to_indices.get(analogy.a)]\n",
    "     \n",
    "    '''\n",
    "        for i in range(len(analogy.b_set)):\n",
    "            temp[i] = embeddings[words_to_indices.get(analogy.b_set[i])]\n",
    "        temp += embeddings[words_to_indices.get(analogy.c)]\n",
    "        temp -= embeddings[words_to_indices.get(analogy.a)]\n",
    "        all_prediction_matrices.append(temp)\n",
    "        '''\n",
    "    #all_predictions = np.concatenate(all_prediction_matrices, axis=0)\n",
    "    all_distances = sim(embeddings, all_predictions.T)\n",
    "    for analogy in analogies:\n",
    "        start_index = analogy_predictions_indices.get(analogy)[0]\n",
    "        a_ind = words_to_indices.get(analogy.a)\n",
    "        b_indices = [words_to_indices.get(b) for b in analogy.b_set]\n",
    "        c_ind = words_to_indices.get(analogy.c)\n",
    "        for i in range(len(b_indices)):\n",
    "            all_distances[a_ind,  start_index + i] = np.NINF \n",
    "            all_distances[b_indices[i], start_index + i] = np.NINF\n",
    "            all_distances[c_ind, start_index + i] = np.NINF\n",
    "    prediction_indices = np.argmax(all_distances, axis=0)\n",
    "    for analogy in analogies:\n",
    "        start_ind = analogy_predictions_indices[analogy][0]\n",
    "        end_ind = analogy_predictions_indices[analogy][1]\n",
    "        for i in range(start_ind, end_ind):\n",
    "            if indices_to_words.get(prediction_indices[i]) in analogy.solution_set:\n",
    "                num_correct += 1\n",
    "                break\n",
    "    return num_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''construct_analogies takes a list of lines from a BATS file and constructs the corresponding analogy lines.\n",
    "Each of the original line has a pair, e.g. \"cat cats\" or \"dog dogs\" and the corresponding analogy would be \n",
    "\"cat is to cats as dog is to dogs\" '''\n",
    "def construct_analogies(lines):\n",
    "    analogies = []\n",
    "    for i in range(len(lines)):\n",
    "        for j in range(len(lines)):\n",
    "            if i == j:\n",
    "                pass \n",
    "            else:\n",
    "                split_first_line = lines[i].split()\n",
    "                a = split_first_line[0]\n",
    "                b_set = split_first_line[1].split(\"/\")\n",
    "                split_second_line = lines[j].split()\n",
    "                c = split_second_line[0]\n",
    "                solution_set = split_second_line[1].split(\"/\")\n",
    "                analogies.append(BATS_Analogy(a, b_set, c, solution_set))\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogies(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    lines = file.readlines()\n",
    "    analogies = construct_analogies(lines)\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load_vectors takes a filepath to a file containing word vectors as a .txt file\n",
    "of the format word [vector components] e.g. for a 4dimensional vector one line could be\n",
    "\"the 0.004 -10.499 0.000 \\n\"\n",
    "the function returns a numpy array of size vxd where v is the size of the vocabulary and \n",
    "d is the length of each vector. The function also returns two dictionaries, one of which\n",
    "has words as keys and indices as values and the other has indices as keys and words as vectors.\n",
    "'''\n",
    "def load_vectors(filepath, normalize=1):\n",
    "    num_failed = 0\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    file = open(filepath, 'r', encoding='utf-8')\n",
    "    lines = file.readlines()\n",
    "    embeddings = np.ones((len(lines), len(lines[0].split())-1), dtype=np.float32)\n",
    "    for i in range(len(lines)):\n",
    "        try:\n",
    "            vec_info = lines[i].split()\n",
    "            word = vec_info[0]\n",
    "            vec = np.array(vec_info[1:], dtype=np.float32)\n",
    "            #vec_nums = [np.float32(component) for component in vec_info[1:]]\n",
    "           # vec = np.array(vec_nums)\n",
    "            if normalize:\n",
    "                #normalize vectors\n",
    "                vec = vec/(np.linalg.norm(vec))\n",
    "            embeddings[i] = vec\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        except:\n",
    "            num_failed += 1\n",
    "            print(\"ERROR with word \", word)\n",
    "    print(\"Successfully loaded \", 100* (len(lines) - num_failed)/float(len(lines)), \"% of vectors in file: \", filepath)\n",
    "    return embeddings, word_to_index, index_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unsolvable(analogies, word_to_index):\n",
    "    solvable_analogies = []\n",
    "    for analogy in analogies:\n",
    "        a_ind = word_to_index.get(analogy.a)\n",
    "        c_ind = word_to_index.get(analogy.c)\n",
    "        if a_ind == None or c_ind == None:\n",
    "            pass\n",
    "        else:\n",
    "            b_set_known = []\n",
    "            b_all_null = True\n",
    "            for b in analogy.b_set:\n",
    "                if word_to_index.get(b) != None:\n",
    "                    b_all_null = False\n",
    "                    b_set_known.append(b)\n",
    "                else:\n",
    "                    continue\n",
    "            if b_all_null:\n",
    "                pass\n",
    "            else:\n",
    "                solvable_analogies.append(BATS_Analogy(analogy.a, b_set_known, analogy.c, analogy.solution_set))\n",
    "    return solvable_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_path = \"./Pretrained_Vectors/glove.6B/glove.6B.50d/glove.6B.50d.txt\"\n",
    "#embeddings, word_to_index, index_to_word = load_vectors(vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bats_path = \"./BATS_3.0/BATS_3.0/4_Lexicographic_semantics/L03 [hyponyms - misc].txt\"\n",
    "#analogies = get_analogies(bats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogies = remove_unsolvable(analogies, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_on_analogies(analogies, embeddings, word_to_index, index_to_word, sim, batch_size=100):\n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    while i < len(analogies):\n",
    "        end_ind = min(i+batch_size, len(analogies))\n",
    "        batch = analogies[i:end_ind]\n",
    "        total_correct += calculate_batch_accuracy(batch, embeddings, word_to_index, index_to_word, sim)\n",
    "        print(\"Completed batch: \", i, \"through \", end_ind )\n",
    "        i += batch_size\n",
    "    return total_correct, len(analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_analogy_tests(test_filepaths, model_name, embeddings, word_to_index, index_to_word, sim):\n",
    "    results = []\n",
    "    for filepath in test_filepaths:\n",
    "        temp = filepath.split(\"/\")\n",
    "        filename = temp[-1]\n",
    "        recorded_file = False\n",
    "        batch_size = 4096\n",
    "        analogies = get_analogies(filepath)\n",
    "        analogies = remove_unsolvable(analogies, word_to_index)\n",
    "        while not recorded_file:\n",
    "            try:\n",
    "                test_correct, test_attempted = compute_accuracy_on_analogies(analogies, embeddings, word_to_index, index_to_word, sim, batch_size)\n",
    "                results.append({'Model': model_name, 'Test': filename, 'Total Correct': test_correct, 'Total Attempted': test_attempted})\n",
    "                recorded_file = True\n",
    "                print(\"Successfully ran analogies from\", filename, \"\\n\")\n",
    "            except:\n",
    "                if batch_size == 1:\n",
    "                    print(\"Failed to Solve Analogies with batch_size = 1 on file\", filename)\n",
    "                    break\n",
    "                print(\"\\nFailed to solve with batch_size = \", batch_size, \"\\n\")\n",
    "                batch_size = batch_size/2\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_filepaths(root_directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_model(vector_path, test_paths, sim, model_name, output_dir):\n",
    "    print(\"Attempting to load embeddings from \", vector_path)\n",
    "    embeddings, word_to_index, index_to_word = load_vectors(vector_path)\n",
    "    results = record_analogy_tests(test_paths, model_name, embeddings, word_to_index, index_to_word, sim)\n",
    "    output_loc = output_dir + model_name\n",
    "    '''\n",
    "    try:\n",
    "        with open(output_loc, 'w') as json_file:\n",
    "            json.dump(results, json_file)\n",
    "        json_file.close()\n",
    "        return True\n",
    "    except:\n",
    "        return results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_models(vector_root_dir, test_root_dir, sim, output_dir):\n",
    "    test_paths = get_list_of_filepaths(test_root_dir)[1:]\n",
    "    vector_paths = get_list_of_filepaths(vector_root_dir)[9:]   #temp hardcode,cuz alreaady did one experiment\n",
    "    failed_to_test = []\n",
    "    results = []\n",
    "    for vector_path in vector_paths:\n",
    "        print(\"Current Model at\", vector_path, \"\\n\")\n",
    "        temp = vector_path.split('\\\\')\n",
    "        model_name = temp[-1]\n",
    "        try:\n",
    "            vec_results = run_one_model(vector_path, test_paths, sim, model_name, output_dir)\n",
    "            if vec_results != True:\n",
    "                results.append(vec_results)\n",
    "                print(\"Did not save results, but did append to results list.\\n\")\n",
    "        else:\n",
    "            print(\"Successfully saved results for \", model_name, \"to directory \", output_dir, \"\\n\")\n",
    "        except:\n",
    "            print(\"\\nFAILED TO TEST EMBEDDINGS FROM\", vector_path, \"\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATS_loc = \"./BATS_3.0/BATS_3.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_loc = \"./Pretrained_Vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = \"./Experimental_Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model at ./Pretrained_Vectors\\glove.twitter.27B\\glove.twitter.27B.50d.txt \n",
      "\n",
      "Attempting to load embeddings from  ./Pretrained_Vectors\\glove.twitter.27B\\glove.twitter.27B.50d.txt\n",
      "ERROR with word  0.45973\n",
      "Successfully loaded  99.99991621380227 % of vectors in file:  ./Pretrained_Vectors\\glove.twitter.27B\\glove.twitter.27B.50d.txt\n",
      "Completed batch:  0 through  2450\n",
      "Successfully ran analogies from BATS_3.0\\1_Inflectional_morphology\\I01 [noun - plural_reg].txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#results = run_one_model(vector1, BATS_loc, cosine_similarity_normalized, model_name, output_location)\n",
    "results_all = run_all_models(vector_loc, BATS_loc, cosine_similarity_normalized, output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = \"glove.6B.50d_experiment_results.txt\"\n",
    "with open(output_1, 'w') as json_file:\n",
    "    json.dump(results, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_1, 'r') as json_file:\n",
    "    read_results = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests_completed = 0\n",
    "for temp in read_results:\n",
    "    num_tests_completed += 1\n",
    "    print(temp, \"\\n\")\n",
    "print(num_tests_completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = int(10)\n",
    "var2 = 0.003\n",
    "print(\"test{0} solved {1:.4f}\".format(var1, var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"ab/cdef/fdfg\"\n",
    "test = string.split(\"/\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_vectors(filepath, normalize=1):\n",
    "    num_failed = 0\n",
    "    words_failed = []\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    file = open(filepath, 'r', encoding='utf-8')\n",
    "    lines = file.readlines()\n",
    "    embeddings = np.ones((len(lines), len(lines[0].split())-1), dtype=np.float32)\n",
    "    for i in range(len(lines)):\n",
    "        try:\n",
    "            vec_info = lines[i].split()\n",
    "            word = vec_info[0]\n",
    "            vec = np.array(vec_info[1:], dtype=np.float32)\n",
    "            #vec_nums = [np.float32(component) for component in vec_info[1:]]\n",
    "           # vec = np.array(vec_nums)\n",
    "            if normalize:\n",
    "                #normalize vectors\n",
    "                vec = vec/(np.linalg.norm(vec))\n",
    "            embeddings[i] = vec\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        except:\n",
    "            num_failed += 1\n",
    "            words_failed.append(word)\n",
    "    print(\"Successfully loaded \", 100* (len(lines) - num_failed)/float(len(lines)), \"% of vectors in file: \", filepath)\n",
    "    return embeddings, word_to_index, index_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_path = \"./Pretrained_Vectors/glove.twitter.27B/glove.twitter.27B.25d.txt\"\n",
    "vecs_test, vecs_test_word_to_index, vecs_test_index_to_word = test_load_vectors(test_vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecs_test[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mismatches = 0\n",
    "for word in vecs_test_word_to_index:\n",
    "    index = vecs_test_word_to_index.get(word)\n",
    "    if word != vecs_test_index_to_word.get(index):\n",
    "        num_mismatches += 1\n",
    "print(\"Found \", num_mismatches, \"Mismatched index word pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as json_file:\n",
    "    json.dump(results, json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as json_file:\n",
    "    test_open = json.load(json_file)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loc = output_location + \"test.txt\"\n",
    "with open(test_loc, 'w') as test_write:\n",
    "    test_write.write(\"Hi\")\n",
    "test_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GloVe_Project_ML_Opt_2020",
   "language": "python",
   "name": "glove_project_ml_opt_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
