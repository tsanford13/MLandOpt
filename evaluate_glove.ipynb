{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A BATS_Analogy is an analogy from the BATS test set. It may allow\n",
    "multiple correct answers! While most analogies are of the form\n",
    "a:b::c:d (read as \"a is to be as c is to d\") a BATS Analogy is of the form\n",
    "a:b1,b2,...,bn::c:d,d1,d2,...,dn (read as \"a is to b1 or b2 or ... or bn as \n",
    "c is to d1 or d2 or ... or dn\") an example would be:\n",
    "\"snake is to nest/pit/acquarium as tiger is to den/cage.\"'''\n",
    "class BATS_Analogy:\n",
    "    def __init__(self, a, b_set, c, d_set):\n",
    "        self.a = a\n",
    "        self.b_set = b_set\n",
    "        self.c = c\n",
    "        self.solution_set = d_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''takes two matrices A and B and returns \n",
    "A@B. If A and B are normalized then this is equivalent\n",
    "to calculating the cosine similarity of each row of A with\n",
    "each column of B. i.e. the cosine similarity of the ith row of A\n",
    "with the jth column of B is given by (A@B)[i,j]'''\n",
    "def cosine_similarity_normalized(A, B):\n",
    "    return A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a list of analogies, a numpy array of word embeddings, 2 dictionaries\n",
    "which are used to get the embedding matrix index corresponding to a specific word and vice versa,\n",
    "and the last argument sim is a similarity function which takes two numpy arrays as arguments and\n",
    "calculates a \"similarity matrix\" whose i,j, component corresponds to the similarity of the ith row of the first matrix\n",
    "with the jth column of the second matrix. An example of a sim function is cosine similarity.\n",
    "The function returns the number of correctly answered analogies over the entire batch.'''\n",
    "def calculate_batch_accuracy(analogies, embeddings, words_to_indices, indices_to_words, sim):\n",
    "    num_correct = 0\n",
    "    analogy_predictions_indices = dict()\n",
    "    start = 0\n",
    "    all_prediction_matrices = []\n",
    "    num_predictions = 0\n",
    "    #loop figures out how large matrix needs to be and which indices will correspond to which vectors\n",
    "    for analogy in analogies:\n",
    "        num_predictions += len(analogy.b_set)\n",
    "        end = start + len(analogy.b_set)\n",
    "        analogy_predictions_indices[analogy] = (start,end)\n",
    "        start = end\n",
    "        \n",
    "    all_predictions = np.empty((num_predictions, embeddings.shape[1]), dtype=np.float32)\n",
    "    #after allocating space for matrix, loop is used to fill in values needed to make predictions\n",
    "    for analogy in analogies:\n",
    "        offset = analogy_predictions_indices.get(analogy)[0]\n",
    "        for i in range(analogy_predictions_indices.get(analogy)[1] - offset):\n",
    "            all_predictions[i + offset] = embeddings[words_to_indices.get(analogy.b_set[i])]\n",
    "        start = analogy_predictions_indices.get(analogy)[0]\n",
    "        end = analogy_predictions_indices.get(analogy)[1]\n",
    "        all_predictions[range(start, end)] += embeddings[words_to_indices.get(analogy.c)]\n",
    "        all_predictions[range(start, end)] -= embeddings[words_to_indices.get(analogy.a)]\n",
    "     \n",
    "    #all_distances calculates the value of sim(prediction, word) for all predictions made and words in vocabulary\n",
    "    all_distances = sim(embeddings, all_predictions.T)\n",
    "    \n",
    "    #next nested loop sets values for a,b,c for each analogy to negative infinity. This prevents model from answering\n",
    "    #the analogy of the form a:b::c:d with any of a,b, or c. \n",
    "    for analogy in analogies:\n",
    "        start_index = analogy_predictions_indices.get(analogy)[0]\n",
    "        a_ind = words_to_indices.get(analogy.a)\n",
    "        b_indices = [words_to_indices.get(b) for b in analogy.b_set]\n",
    "        c_ind = words_to_indices.get(analogy.c)\n",
    "        for i in range(len(b_indices)):\n",
    "            all_distances[a_ind,  start_index + i] = np.NINF \n",
    "            all_distances[b_indices[i], start_index + i] = np.NINF\n",
    "            all_distances[c_ind, start_index + i] = np.NINF\n",
    "    \n",
    "    #prediction_indices has index of each prediction made by the model. Used in last loop to find number answered correctly.\n",
    "    prediction_indices = np.argmax(all_distances, axis=0)\n",
    "    for analogy in analogies:\n",
    "        start_ind = analogy_predictions_indices[analogy][0]\n",
    "        end_ind = analogy_predictions_indices[analogy][1]\n",
    "        for i in range(start_ind, end_ind):\n",
    "            if indices_to_words.get(prediction_indices[i]) in analogy.solution_set:\n",
    "                num_correct += 1\n",
    "                break\n",
    "    return num_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''construct_analogies takes a list of lines from a BATS file and constructs the corresponding analogy lines.\n",
    "Each of the original line has a pair, e.g. \"cat cats\" or \"dog dogs\" and the corresponding analogy would be \n",
    "\"cat is to cats as dog is to dogs\" '''\n",
    "def construct_analogies(lines):\n",
    "    analogies = []\n",
    "    for i in range(len(lines)):\n",
    "        for j in range(len(lines)):\n",
    "            if i == j:\n",
    "                pass \n",
    "            else:\n",
    "                split_first_line = lines[i].split()\n",
    "                a = split_first_line[0]\n",
    "                b_set = split_first_line[1].split(\"/\")\n",
    "                split_second_line = lines[j].split()\n",
    "                c = split_second_line[0]\n",
    "                solution_set = split_second_line[1].split(\"/\")\n",
    "                analogies.append(BATS_Analogy(a, b_set, c, solution_set))\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a filepath get_analogies returns a list of BATS_Analogy's \n",
    "def get_analogies(filepath):\n",
    "    file = open(filepath, 'r')\n",
    "    lines = file.readlines()\n",
    "    analogies = construct_analogies(lines)\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load_vectors takes a filepath to a file containing word vectors as a .txt file\n",
    "of the format word [vector components] e.g. for a 4dimensional vector one line could be\n",
    "\"the 0.004 -10.499 0.000 \\n\"\n",
    "the function returns a numpy array of size vxd where v is the size of the vocabulary and \n",
    "d is the length of each vector. The function also returns two dictionaries, one of which\n",
    "has words as keys and indices as values and the other has indices as keys and words as vectors.\n",
    "'''\n",
    "def load_vectors(filepath, normalize=1):\n",
    "    num_failed = 0\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    file = open(filepath, 'r', encoding='utf-8')\n",
    "    lines = file.readlines()\n",
    "    embeddings = np.ones((len(lines), len(lines[0].split())-1), dtype=np.float32)\n",
    "    for i in range(len(lines)):\n",
    "        try:\n",
    "            vec_info = lines[i].split()\n",
    "            word = vec_info[0]\n",
    "            vec = np.array(vec_info[1:], dtype=np.float32)\n",
    "            #vec_nums = [np.float32(component) for component in vec_info[1:]]\n",
    "           # vec = np.array(vec_nums)\n",
    "            if normalize:\n",
    "                #normalize vectors\n",
    "                vec = vec/(np.linalg.norm(vec))\n",
    "            embeddings[i] = vec\n",
    "            word_to_index[word] = i \n",
    "            index_to_word[i] = word\n",
    "        except:\n",
    "            num_failed += 1\n",
    "            print(\"ERROR with word \", word)\n",
    "    print(\"Successfully loaded \", 100* (len(lines) - num_failed)/float(len(lines)), \"% of vectors in file: \", filepath)\n",
    "    return embeddings, word_to_index, index_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogies which require vocabulary which the model doesn't know are removed. \n",
    "def remove_unsolvable(analogies, word_to_index):\n",
    "    solvable_analogies = []\n",
    "    for analogy in analogies:\n",
    "        a_ind = word_to_index.get(analogy.a)\n",
    "        c_ind = word_to_index.get(analogy.c)\n",
    "        if a_ind == None or c_ind == None:\n",
    "            pass\n",
    "        else:\n",
    "            b_set_known = []\n",
    "            b_all_null = True\n",
    "            for b in analogy.b_set:\n",
    "                if word_to_index.get(b) != None:\n",
    "                    b_all_null = False\n",
    "                    b_set_known.append(b)\n",
    "                else:\n",
    "                    continue\n",
    "            if b_all_null:\n",
    "                pass\n",
    "            else:\n",
    "                solvable_analogies.append(BATS_Analogy(analogy.a, b_set_known, analogy.c, analogy.solution_set))\n",
    "    return solvable_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_path = \"./Pretrained_Vectors/glove.6B/glove.6B.50d/glove.6B.50d.txt\"\n",
    "#embeddings, word_to_index, index_to_word = load_vectors(vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bats_path = \"./BATS_3.0/BATS_3.0/4_Lexicographic_semantics/L03 [hyponyms - misc].txt\"\n",
    "#analogies = get_analogies(bats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogies = remove_unsolvable(analogies, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a list of analogies, an embeddings array, 2 dictionaries used for translating between words\n",
    "and indexes and vice versa, a similarity function used to pick the best prediction, and a batch_size which defines how\n",
    "many analogies to attempt at a time. It returns the number of correctly solved analogies and the number of analogies attempted.'''\n",
    "def compute_accuracy_on_analogies(analogies, embeddings, word_to_index, index_to_word, sim, batch_size=100):\n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    while i < len(analogies):\n",
    "        end_ind = min(i+batch_size, len(analogies))\n",
    "        batch = analogies[i:end_ind]\n",
    "        total_correct += calculate_batch_accuracy(batch, embeddings, word_to_index, index_to_word, sim)\n",
    "        print(\"Completed batch: \", i, \"through \", end_ind )\n",
    "        i += batch_size\n",
    "    return total_correct, len(analogies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function takes a list of filepaths to test files, a model name and the corresponding\n",
    "embeddings matrix, 2 dictionaries used to translate between words and indexes, and a similarity function used to pick \n",
    "optimal predictions.'''\n",
    "def record_analogy_tests(test_filepaths, model_name, embeddings, word_to_index, index_to_word, sim):\n",
    "    results = []\n",
    "    for filepath in test_filepaths:\n",
    "        temp = filepath.split(\"/\")\n",
    "        filename = temp[-1]\n",
    "        recorded_file = False\n",
    "        batch_size = 4096\n",
    "        analogies = get_analogies(filepath)\n",
    "        analogies = remove_unsolvable(analogies, word_to_index)\n",
    "        while not recorded_file:\n",
    "            #try except loop enables us to solve as many tests as possible and ignore ones which fail\n",
    "            try:\n",
    "                test_correct, test_attempted = compute_accuracy_on_analogies(analogies, embeddings, word_to_index, index_to_word, sim, batch_size)\n",
    "                results.append({'Model': model_name, 'Test': filename, 'Total Correct': test_correct, 'Total Attempted': test_attempted})\n",
    "                recorded_file = True\n",
    "                print(\"Successfully ran analogies from\", filename, \"\\n\")\n",
    "            except:\n",
    "                if batch_size == 1:\n",
    "                    print(\"Failed to Solve Analogies with batch_size = 1 on file\", filename)\n",
    "                    break\n",
    "                print(\"\\nFailed to solve with batch_size = \", batch_size, \"\\n\")\n",
    "                batch_size = batch_size/2\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function takes a filepath to a root directory and returns a list of filepaths to all files in the root\n",
    "directory and subdirectories.'''\n",
    "def get_list_of_filepaths(root_directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function takes a filepath to the file which stores the word embedding matrix, a list of filepaths to the test files,\n",
    "a similarity function, a model name, and a path to an output directory in which to save the results. Note that the \n",
    "output directory must be created before this function is called. Otherwise it will not write the experimental results\n",
    "to a file, it will only return them as a dictionary. When it does successfully write to a file it uses JSON format.'''\n",
    "def run_one_model(vector_path, test_paths, sim, model_name, output_dir):\n",
    "    print(\"Attempting to load embeddings from \", vector_path)\n",
    "    embeddings, word_to_index, index_to_word = load_vectors(vector_path)\n",
    "    results = record_analogy_tests(test_paths, model_name, embeddings, word_to_index, index_to_word, sim)\n",
    "    output_loc = output_dir + model_name\n",
    "    try:\n",
    "        with open(output_loc, 'w') as json_file:\n",
    "            json.dump(results, json_file)\n",
    "        json_file.close()\n",
    "        return True\n",
    "    except:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function accepts a filepath to a root directory in which all the word embedding files are stored (can also be stored\n",
    "in subdirectories), a filepath to a root directory which holds all the test files (again tests can be stored in subdirectories),\n",
    "a similarity function used to choose predictions, and a path to an output directory in which to save the results. Note the \n",
    "output directory must be created before it is written to. If output file does not exist then no files are written and the \n",
    "function returns a list of dictionaries which contain the results. Dictionaries are used because they can be easily written\n",
    "to JSON files (which is the format used throughout this project to store experimental results.)\n",
    "'''\n",
    "def run_all_models(vector_root_dir, test_root_dir, sim, output_dir):\n",
    "    test_paths = get_list_of_filepaths(test_root_dir)[1:]\n",
    "    vector_paths = get_list_of_filepaths(vector_root_dir)[5:]   #temp hardcode,cuz alreaady did 5 experiment\n",
    "    failed_to_test = []\n",
    "    results = []\n",
    "    for vector_path in vector_paths:\n",
    "        print(\"Current Model at\", vector_path, \"\\n\")\n",
    "        temp = vector_path.split('\\\\')\n",
    "        model_name = temp[-1]\n",
    "        try:\n",
    "            vec_results = run_one_model(vector_path, test_paths, sim, model_name, output_dir)\n",
    "            if vec_results != True:\n",
    "                results.append(vec_results)\n",
    "                print(\"Did not save results, but did append to results list.\\n\")\n",
    "            else:\n",
    "                print(\"Successfully saved results for \", model_name, \"to directory \", output_dir, \"\\n\")\n",
    "        except:\n",
    "            print(\"\\nFAILED TO TEST EMBEDDINGS FROM\", vector_path, \"\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path to BATS test files\n",
    "BATS_loc = \"./BATS_3.0/BATS_3.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define path to Vector files\n",
    "vector_loc = \"./Pretrained_Vectors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path to output location. Directory must exist already in order to be written to!!!\n",
    "output_location = \"./Experimental_Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model at ./Pretrained_Vectors\\glove.840B.300d\\glove.840B.300d.txt \n",
      "\n",
      "Attempting to load embeddings from  ./Pretrained_Vectors\\glove.840B.300d\\glove.840B.300d.txt\n",
      "ERROR with word  .\n",
      "ERROR with word  at\n",
      "ERROR with word  0.20785\n",
      "ERROR with word  .\n",
      "ERROR with word  to\n",
      "ERROR with word  .\n",
      "ERROR with word  .\n",
      "ERROR with word  email\n",
      "ERROR with word  0.39511\n",
      "ERROR with word  or\n",
      "ERROR with word  0.13211\n",
      "ERROR with word  contact\n",
      "ERROR with word  -0.38024\n",
      "ERROR with word  -0.0033421\n",
      "ERROR with word  Email\n",
      "ERROR with word  on\n",
      "ERROR with word  0.14608\n",
      "ERROR with word  -0.36288\n",
      "ERROR with word  At\n",
      "ERROR with word  by\n",
      "ERROR with word  in\n",
      "ERROR with word  0.5478\n",
      "ERROR with word  emailing\n",
      "ERROR with word  Contact\n",
      "ERROR with word  0.59759\n",
      "ERROR with word  at\n",
      "ERROR with word  â€¢\n",
      "ERROR with word  at\n",
      "ERROR with word  is\n",
      "Successfully loaded  99.99867942734505 % of vectors in file:  ./Pretrained_Vectors\\glove.840B.300d\\glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "#run all experiments and store results (WARNING THIS MAY TAKE SEVERAL DAYS TO FINISH)\n",
    "results_all = run_all_models(vector_loc, BATS_loc, cosine_similarity_normalized, output_location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GloVe_Project_ML_Opt_2020",
   "language": "python",
   "name": "glove_project_ml_opt_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
